{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline  clickbait\n",
      "0                                 Should I Get Bings       True\n",
      "1      Which TV Female Friend Group Do You Belong In       True\n",
      "2  The New \"Star Wars: The Force Awakens\" Trailer...       True\n",
      "3  This Vine Of New York On \"Celebrity Big Brothe...       True\n",
      "4  A Couple Did A Stunning Photo Shoot With Their...       True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_clickbait_dataset.csv' with the path to your dataset\n",
    "dataset = pd.read_csv('clickbait_data.csv')\n",
    "\n",
    "# Turn the clickbait column into a boolean column\n",
    "dataset['clickbait'] = dataset['clickbait'].astype(bool)\n",
    "\n",
    "# Display information about the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_number_columns(headline: str) -> pd.Series:\n",
    "    # Add columns for headlines with no numbers, numbers at the start, and numbers in the middle\n",
    "    no_number = not bool(re.search(r'\\d+', headline))\n",
    "    number_start = bool(re.match(r'^\\d+', headline))\n",
    "    number_middle = bool(re.search(r'\\d+', headline) and not number_start)\n",
    "    \n",
    "    return pd.Series([no_number, number_start, number_middle])\n",
    "\n",
    "# Apply the add_number_columns function to each headline in the dataset\n",
    "dataset[['NoNumber', 'NumberStart', 'NumberMiddle']] = dataset['headline'].apply(add_number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_character_columns(headline: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Add columns for special characters '-', '=', \"'\", and '.'.\n",
    "    These characters were chosen because they are the top 4 special characters\n",
    "    found in clickbait and non-clickbait headlines, as shown in the plot above.\n",
    "    \"\"\"\n",
    "    has_minus = '-' in headline\n",
    "    has_equals = '=' in headline\n",
    "    has_apostrophe = \"'\" in headline\n",
    "    has_period = '.' in headline\n",
    "    \n",
    "    return pd.Series([has_minus, has_equals, has_apostrophe, has_period])\n",
    "\n",
    "# Apply the add_special_character_columns function to each headline in the dataset\n",
    "dataset[['HasMinus', 'HasEquals', 'HasApostrophe', 'HasPeriod']] = dataset['headline'].apply(add_special_character_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "# Tokenize and POS tag each headline\n",
    "tokens = dataset['headline'].apply(nltk.word_tokenize)\n",
    "pos_tags = tokens.apply(lambda tokens: [tag for word, tag in nltk.pos_tag(tokens)])\n",
    "\n",
    "# Define the POS tags and their descriptions\n",
    "pos_tags_dict = {\n",
    "    'CC': 'coordinating conjunction',\n",
    "    'CD': 'cardinal digit',\n",
    "    'DT': 'determiner',\n",
    "    'EX': 'existential there',\n",
    "    'FW': 'foreign word',\n",
    "    'IN': 'preposition/subordinating conjunction',\n",
    "    'JJ': 'adjective',\n",
    "    'JJR': 'adjective, comparative',\n",
    "    'JJS': 'adjective, superlative',\n",
    "    'LS': 'list marker',\n",
    "    'MD': 'modal',\n",
    "    'NN': 'noun, singular',\n",
    "    'NNS': 'noun plural',\n",
    "    'NNP': 'proper noun, singular',\n",
    "    'NNPS': 'proper noun, plural',\n",
    "    'PDT': 'predeterminer',\n",
    "    'POS': 'possessive ending',\n",
    "    'PRP': 'personal pronoun',\n",
    "    'PRP$': 'possessive pronoun',\n",
    "    'RB': 'adverb',\n",
    "    'RBR': 'adverb, comparative',\n",
    "    'RBS': 'adverb, superlative',\n",
    "    'RP': 'particle',\n",
    "    'TO': 'to',\n",
    "    'UH': 'interjection',\n",
    "    'VB': 'verb, base form',\n",
    "    'VBD': 'verb, past tense',\n",
    "    'VBG': 'verb, gerund/present participle',\n",
    "    'VBN': 'verb, past participle',\n",
    "    'VBP': 'verb, sing. present, non-3d',\n",
    "    'VBZ': 'verb, 3rd person sing. present',\n",
    "    'WDT': 'wh-determiner',\n",
    "    'WP': 'wh-pronoun',\n",
    "    'WP$': 'possessive wh-pronoun',\n",
    "    'WRB': 'wh-adverb'\n",
    "}\n",
    "\n",
    "# Create a DataFrame to store the POS tag columns\n",
    "pos_columns = pd.DataFrame(index=dataset.index)\n",
    "\n",
    "# Add columns for each POS tag and initialize them to False\n",
    "for tag, description in pos_tags_dict.items():\n",
    "    pos_columns[description] = False\n",
    "\n",
    "# Set the corresponding POS tag columns to True for each headline\n",
    "for i, tags in enumerate(pos_tags):\n",
    "    for tag in tags:\n",
    "        if tag in pos_tags_dict:\n",
    "            pos_columns.at[i, pos_tags_dict[tag]] = True\n",
    "\n",
    "# Add the POS tag columns to the original dataset\n",
    "dataset = pd.concat([dataset, pos_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'headline' column before calculating the correlation matrix\n",
    "correlation_matrix = dataset.drop(columns=['headline']).corr()\n",
    "\n",
    "# Get the correlation of each feature with the 'clickbait' column\n",
    "correlation_with_clickbait = correlation_matrix['clickbait'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Remove the 'clickbait' column from the list\n",
    "correlation_with_clickbait = correlation_with_clickbait.drop('clickbait')\n",
    "\n",
    "# Get the top 20 features with the highest correlation\n",
    "top_20_features = correlation_with_clickbait.index[:6]\n",
    "\n",
    "# Keep only the top 5 features + 'clickbait' and 'headline' in the dataset\n",
    "dataset = dataset[['headline', 'clickbait'] + list(correlation_with_clickbait.index[:6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>personal pronoun</th>\n",
       "      <th>NumberStart</th>\n",
       "      <th>noun, singular</th>\n",
       "      <th>determiner</th>\n",
       "      <th>NoNumber</th>\n",
       "      <th>cardinal digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait  \\\n",
       "0                                 Should I Get Bings       True   \n",
       "1      Which TV Female Friend Group Do You Belong In       True   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...       True   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...       True   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...       True   \n",
       "\n",
       "   personal pronoun  NumberStart  noun, singular  determiner  NoNumber  \\\n",
       "0              True        False           False       False      True   \n",
       "1              True        False            True       False      True   \n",
       "2              True        False            True        True      True   \n",
       "3             False        False           False        True      True   \n",
       "4              True        False            True        True      True   \n",
       "\n",
       "   cardinal digit  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.9193 - loss: 0.2077 - val_accuracy: 0.9798 - val_loss: 0.0541\n",
      "Epoch 2/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9925 - loss: 0.0233 - val_accuracy: 0.9796 - val_loss: 0.0560\n",
      "Epoch 3/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9979 - loss: 0.0058 - val_accuracy: 0.9787 - val_loss: 0.0933\n",
      "Epoch 4/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0015 - val_accuracy: 0.9792 - val_loss: 0.0991\n",
      "Epoch 5/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9998 - loss: 4.8460e-04 - val_accuracy: 0.9794 - val_loss: 0.1221\n",
      "Epoch 6/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0017 - val_accuracy: 0.9760 - val_loss: 0.1143\n",
      "Epoch 7/7\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0013 - val_accuracy: 0.9781 - val_loss: 0.1346\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9790 - loss: 0.1287\n",
      "Test Loss: 0.1436, Test Accuracy: 0.9767\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Clickbait       0.98      0.98      0.98      2395\n",
      "    Clickbait       0.98      0.98      0.98      2405\n",
      "\n",
      "     accuracy                           0.98      4800\n",
      "    macro avg       0.98      0.98      0.98      4800\n",
      " weighted avg       0.98      0.98      0.98      4800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense, Input, Concatenate, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "max_words = 10000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenizer erstellen und auf Daten fitten\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataset['headline'])\n",
    "\n",
    "# Textdaten vorbereiten\n",
    "sequences = tokenizer.texts_to_sequences(dataset['headline'])\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "# Zusätzliche Features vorbereiten\n",
    "extra_features = dataset[['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                            'determiner', 'NoNumber', 'cardinal digit']].astype(float)\n",
    "\n",
    "# Normierung der numerischen Features\n",
    "scaler = StandardScaler()\n",
    "extra_features = scaler.fit_transform(extra_features)\n",
    "\n",
    "# Labels\n",
    "labels = dataset['clickbait']\n",
    "\n",
    "# Aufteilen in Training (70%), Temp (30%)\n",
    "X_text_train, X_text_temp, X_extra_train, X_extra_temp, y_train, y_temp = train_test_split(\n",
    "    padded, extra_features, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Aufteilen Temp in Validierung (15%) und Test (15%)\n",
    "X_text_val, X_text_test, X_extra_val, X_extra_test, y_val, y_test = train_test_split(\n",
    "    X_text_temp, X_extra_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# **Modell mit zwei Eingängen definieren**\n",
    "# Input 1: Textdaten\n",
    "input_text = Input(shape=(padded.shape[1],), name=\"text_input\")\n",
    "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(input_text)\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "# Input 2: Zusätzliche numerische Features\n",
    "input_extra = Input(shape=(extra_features.shape[1],), name=\"extra_input\")\n",
    "extra_dense = Dense(32, activation='relu')(input_extra)\n",
    "\n",
    "# Zusammenführen beider Pfade\n",
    "concatenated = Concatenate()([pooling_layer, lstm_layer, extra_dense])\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "dropout_layer = Dropout(0.3)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# Modell erstellen\n",
    "model = Model(inputs=[input_text, input_extra], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modell trainieren mit expliziter Validierung\n",
    "model.fit(\n",
    "    [X_text_train, X_extra_train], y_train,\n",
    "    epochs=7,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_text_val, X_extra_val], y_val)\n",
    ")\n",
    "\n",
    "# Modell evaluieren auf dem Test-Set\n",
    "test_loss, test_accuracy = model.evaluate([X_text_test, X_extra_test], y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Klassifikationsbericht für das Test-Set\n",
    "predictions = model.predict([X_text_test, X_extra_test])\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "print(classification_report(y_test, predicted_labels, target_names=['Non-Clickbait', 'Clickbait']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clickbait(headline):\n",
    "    # Tokenize and POS tag the input headline\n",
    "    tokens = nltk.word_tokenize(headline)\n",
    "    pos_tags = [tag for word, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "    # Create a DataFrame with the same structure as the dataset\n",
    "    test_data = pd.DataFrame(index=[0])\n",
    "    test_data['headline'] = headline\n",
    "\n",
    "    # Add columns for the selected POS tags and initialize them to False\n",
    "    selected_tags = ['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                     'determiner', 'NoNumber', 'cardinal digit']\n",
    "    for tag in selected_tags:\n",
    "        test_data[tag] = False\n",
    "\n",
    "    # Check if the headline starts with a number and set NumberStart to True if it does\n",
    "    if re.match(r'^\\d+', headline):\n",
    "        test_data.at[0, 'NumberStart'] = True\n",
    "\n",
    "    # Check if the headline does not contain any numbers and set NoNumber to True if it doesn't\n",
    "    if not bool(re.search(r'\\d+', headline)):\n",
    "        test_data.at[0, 'NoNumber'] = True\n",
    "\n",
    "    # Set the corresponding POS tag columns to True if the tag is present in the headline\n",
    "    for tag in pos_tags:\n",
    "        if tag in pos_tags_dict and pos_tags_dict[tag] in selected_tags:\n",
    "            test_data.at[0, pos_tags_dict[tag]] = True\n",
    "\n",
    "    # Tokenize and pad the input headline\n",
    "    sequence = tokenizer.texts_to_sequences([headline])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=padded.shape[1])\n",
    "\n",
    "    # Prepare additional features\n",
    "    extra_features = test_data[['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                                'determiner', 'NoNumber', 'cardinal digit']].astype(float)\n",
    "    extra_features = scaler.transform(extra_features)\n",
    "\n",
    "    # Predict the probability of clickbait\n",
    "    prediction = model.predict([padded_sequence, extra_features])\n",
    "\n",
    "    # Return the prediction as a percentage\n",
    "    print(f'The model predicts a {prediction[0][0] * 100:.2f}% chance that the headline is clickbait.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "The model predicts a 16.38% chance that the headline is clickbait.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "The model predicts a 100.00% chance that the headline is clickbait.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "The model predicts a 2.96% chance that the headline is clickbait.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "The model predicts a 100.00% chance that the headline is clickbait.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model with a sample clickbait headline\n",
    "predict_clickbait(\"10 people dead after a plane crash in the ocean\") \n",
    "predict_clickbait(\"You won't believe what happened next!\")\n",
    "predict_clickbait(\"Introduction to Python programming\")\n",
    "predict_clickbait(\"Why you should never eat at this restaurant again\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
