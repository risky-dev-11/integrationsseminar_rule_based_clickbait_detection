{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline  clickbait\n",
      "0                                 Should I Get Bings       True\n",
      "1      Which TV Female Friend Group Do You Belong In       True\n",
      "2  The New \"Star Wars: The Force Awakens\" Trailer...       True\n",
      "3  This Vine Of New York On \"Celebrity Big Brothe...       True\n",
      "4  A Couple Did A Stunning Photo Shoot With Their...       True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_clickbait_dataset.csv' with the path to your dataset\n",
    "dataset = pd.read_csv('clickbait_data.csv')\n",
    "\n",
    "# Turn the clickbait column into a boolean column\n",
    "dataset['clickbait'] = dataset['clickbait'].astype(bool)\n",
    "\n",
    "# Display information about the dataset\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def add_number_columns(headline: str) -> pd.Series:\n",
    "    # Add columns for headlines with no numbers, numbers at the start, and numbers in the middle\n",
    "    no_number = not bool(re.search(r'\\d+', headline))\n",
    "    number_start = bool(re.match(r'^\\d+', headline))\n",
    "    number_middle = bool(re.search(r'\\d+', headline) and not number_start)\n",
    "    \n",
    "    return pd.Series([no_number, number_start, number_middle])\n",
    "\n",
    "# Apply the add_number_columns function to each headline in the dataset\n",
    "dataset[['NoNumber', 'NumberStart', 'NumberMiddle']] = dataset['headline'].apply(add_number_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_character_columns(headline: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Add columns for special characters '-', '=', \"'\", and '.'.\n",
    "    These characters were chosen because they are the top 4 special characters\n",
    "    found in clickbait and non-clickbait headlines, as shown in the plot above.\n",
    "    \"\"\"\n",
    "    has_minus = '-' in headline\n",
    "    has_equals = '=' in headline\n",
    "    has_apostrophe = \"'\" in headline\n",
    "    has_period = '.' in headline\n",
    "    \n",
    "    return pd.Series([has_minus, has_equals, has_apostrophe, has_period])\n",
    "\n",
    "# Apply the add_special_character_columns function to each headline in the dataset\n",
    "dataset[['HasMinus', 'HasEquals', 'HasApostrophe', 'HasPeriod']] = dataset['headline'].apply(add_special_character_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "# Tokenize and POS tag each headline\n",
    "tokens = dataset['headline'].apply(nltk.word_tokenize)\n",
    "pos_tags = tokens.apply(lambda tokens: [tag for word, tag in nltk.pos_tag(tokens)])\n",
    "\n",
    "# Define the POS tags and their descriptions\n",
    "pos_tags_dict = {\n",
    "    'CC': 'coordinating conjunction',\n",
    "    'CD': 'cardinal digit',\n",
    "    'DT': 'determiner',\n",
    "    'EX': 'existential there',\n",
    "    'FW': 'foreign word',\n",
    "    'IN': 'preposition/subordinating conjunction',\n",
    "    'JJ': 'adjective',\n",
    "    'JJR': 'adjective, comparative',\n",
    "    'JJS': 'adjective, superlative',\n",
    "    'LS': 'list marker',\n",
    "    'MD': 'modal',\n",
    "    'NN': 'noun, singular',\n",
    "    'NNS': 'noun plural',\n",
    "    'NNP': 'proper noun, singular',\n",
    "    'NNPS': 'proper noun, plural',\n",
    "    'PDT': 'predeterminer',\n",
    "    'POS': 'possessive ending',\n",
    "    'PRP': 'personal pronoun',\n",
    "    'PRP$': 'possessive pronoun',\n",
    "    'RB': 'adverb',\n",
    "    'RBR': 'adverb, comparative',\n",
    "    'RBS': 'adverb, superlative',\n",
    "    'RP': 'particle',\n",
    "    'TO': 'to',\n",
    "    'UH': 'interjection',\n",
    "    'VB': 'verb, base form',\n",
    "    'VBD': 'verb, past tense',\n",
    "    'VBG': 'verb, gerund/present participle',\n",
    "    'VBN': 'verb, past participle',\n",
    "    'VBP': 'verb, sing. present, non-3d',\n",
    "    'VBZ': 'verb, 3rd person sing. present',\n",
    "    'WDT': 'wh-determiner',\n",
    "    'WP': 'wh-pronoun',\n",
    "    'WP$': 'possessive wh-pronoun',\n",
    "    'WRB': 'wh-adverb'\n",
    "}\n",
    "\n",
    "# Create a DataFrame to store the POS tag columns\n",
    "pos_columns = pd.DataFrame(index=dataset.index)\n",
    "\n",
    "# Add columns for each POS tag and initialize them to False\n",
    "for tag, description in pos_tags_dict.items():\n",
    "    pos_columns[description] = False\n",
    "\n",
    "# Set the corresponding POS tag columns to True for each headline\n",
    "for i, tags in enumerate(pos_tags):\n",
    "    for tag in tags:\n",
    "        if tag in pos_tags_dict:\n",
    "            pos_columns.at[i, pos_tags_dict[tag]] = True\n",
    "\n",
    "# Add the POS tag columns to the original dataset\n",
    "dataset = pd.concat([dataset, pos_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'headline' column before calculating the correlation matrix\n",
    "correlation_matrix = dataset.drop(columns=['headline']).corr()\n",
    "\n",
    "# Get the correlation of each feature with the 'clickbait' column\n",
    "correlation_with_clickbait = correlation_matrix['clickbait'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Remove the 'clickbait' column from the list\n",
    "correlation_with_clickbait = correlation_with_clickbait.drop('clickbait')\n",
    "\n",
    "# Get the top 20 features with the highest correlation\n",
    "top_20_features = correlation_with_clickbait.index[:6]\n",
    "\n",
    "# Keep only the top 5 features + 'clickbait' and 'headline' in the dataset\n",
    "dataset = dataset[['headline', 'clickbait'] + list(correlation_with_clickbait.index[:6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>personal pronoun</th>\n",
       "      <th>NumberStart</th>\n",
       "      <th>noun, singular</th>\n",
       "      <th>determiner</th>\n",
       "      <th>NoNumber</th>\n",
       "      <th>cardinal digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait  \\\n",
       "0                                 Should I Get Bings       True   \n",
       "1      Which TV Female Friend Group Do You Belong In       True   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...       True   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...       True   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...       True   \n",
       "\n",
       "   personal pronoun  NumberStart  noun, singular  determiner  NoNumber  \\\n",
       "0              True        False           False       False      True   \n",
       "1              True        False            True       False      True   \n",
       "2              True        False            True        True      True   \n",
       "3             False        False           False        True      True   \n",
       "4              True        False            True        True      True   \n",
       "\n",
       "   cardinal digit  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 25ms/step - accuracy: 0.9207 - loss: 0.1924 - val_accuracy: 0.9734 - val_loss: 0.0757\n",
      "Epoch 2/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9922 - loss: 0.0220 - val_accuracy: 0.9717 - val_loss: 0.0848\n",
      "Epoch 3/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 0.9720 - val_loss: 0.1143\n",
      "Epoch 4/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9748 - val_loss: 0.1269\n",
      "Epoch 5/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 24ms/step - accuracy: 0.9996 - loss: 0.0011 - val_accuracy: 0.9664 - val_loss: 0.1675\n",
      "Epoch 6/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9527 - val_loss: 0.2785\n",
      "Epoch 7/7\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step - accuracy: 0.9996 - loss: 0.0013 - val_accuracy: 0.9669 - val_loss: 0.2017\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9988 - loss: 0.0060\n",
      "Loss: 0.04131342098116875, Accuracy: 0.9931562542915344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, LSTM, Dense, Input, Concatenate, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Parameters\n",
    "max_words = 10000\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenizer erstellen und auf Daten fitten\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataset['headline'])\n",
    "\n",
    "# Textdaten vorbereiten\n",
    "sequences = tokenizer.texts_to_sequences(dataset['headline'])\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "# Zusätzliche Features vorbereiten\n",
    "extra_features = dataset[['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                          'determiner', 'NoNumber', 'cardinal digit']].astype(float)\n",
    "\n",
    "# Normierung der numerischen Features\n",
    "scaler = StandardScaler()\n",
    "extra_features = scaler.fit_transform(extra_features)\n",
    "\n",
    "# **Modell mit zwei Eingängen definieren**\n",
    "# Input 1: Textdaten\n",
    "input_text = Input(shape=(padded.shape[1],), name=\"text_input\")\n",
    "embedding_layer = Embedding(input_dim=max_words, output_dim=embedding_dim)(input_text)\n",
    "conv_layer = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "lstm_layer = LSTM(128)(embedding_layer)\n",
    "\n",
    "# Input 2: Zusätzliche numerische Features\n",
    "input_extra = Input(shape=(extra_features.shape[1],), name=\"extra_input\")\n",
    "extra_dense = Dense(32, activation='relu')(input_extra)\n",
    "\n",
    "# Zusammenführen beider Pfade\n",
    "concatenated = Concatenate()([pooling_layer, lstm_layer, extra_dense])\n",
    "dense_layer = Dense(64, activation='relu')(concatenated)\n",
    "dropout_layer = Dropout(0.3)(dense_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "# Modell erstellen\n",
    "model = Model(inputs=[input_text, input_extra], outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Modell trainieren\n",
    "model.fit([padded, extra_features], dataset['clickbait'], epochs=7, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Modell evaluieren\n",
    "loss, accuracy = model.evaluate([padded, extra_features], dataset['clickbait'])\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clickbait(headline):\n",
    "    # Tokenize and POS tag the input headline\n",
    "    tokens = nltk.word_tokenize(headline)\n",
    "    pos_tags = [tag for word, tag in nltk.pos_tag(tokens)]\n",
    "\n",
    "    # Create a DataFrame with the same structure as the dataset\n",
    "    test_data = pd.DataFrame(index=[0])\n",
    "    test_data['headline'] = headline\n",
    "\n",
    "    # Add columns for the selected POS tags and initialize them to False\n",
    "    selected_tags = ['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                     'determiner', 'NoNumber', 'cardinal digit']\n",
    "    for tag in selected_tags:\n",
    "        test_data[tag] = False\n",
    "\n",
    "    # Check if the headline starts with a number and set NumberStart to True if it does\n",
    "    if re.match(r'^\\d+', headline):\n",
    "        test_data.at[0, 'NumberStart'] = True\n",
    "\n",
    "    # Check if the headline does not contain any numbers and set NoNumber to True if it doesn't\n",
    "    if not bool(re.search(r'\\d+', headline)):\n",
    "        test_data.at[0, 'NoNumber'] = True\n",
    "\n",
    "    # Set the corresponding POS tag columns to True if the tag is present in the headline\n",
    "    for tag in pos_tags:\n",
    "        if tag in pos_tags_dict and pos_tags_dict[tag] in selected_tags:\n",
    "            test_data.at[0, pos_tags_dict[tag]] = True\n",
    "\n",
    "    # Tokenize and pad the input headline\n",
    "    sequence = tokenizer.texts_to_sequences([headline])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=padded.shape[1])\n",
    "\n",
    "    # Prepare additional features\n",
    "    extra_features = test_data[['personal pronoun', 'NumberStart', 'noun, singular', \n",
    "                                'determiner', 'NoNumber', 'cardinal digit']].astype(float)\n",
    "    extra_features = scaler.transform(extra_features)\n",
    "\n",
    "    # Predict the probability of clickbait\n",
    "    prediction = model.predict([padded_sequence, extra_features])\n",
    "\n",
    "    # Return the prediction as a percentage\n",
    "    print(f'The model predicts a {prediction[0][0] * 100:.2f}% chance that the headline is clickbait.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "The model predicts a 100.00% chance that the headline is clickbait.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model with a sample clickbait headline\n",
    "predict_clickbait(\"You won't believe what happens next!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step\n",
      "5 Error Samples\n",
      "                                                headline  clickbait  \\\n",
      "26098                          When Air Time Is Playtime      False   \n",
      "27455     3 Officers Are Dead After Shootings in Oakland      False   \n",
      "28564      A Magazine Just for You Arrives With Glitches      False   \n",
      "29220  Does It Pay for Studios to Bring in Marketing ...      False   \n",
      "26119  30 Seconds With S. L. Price: Handling Death Fr...      False   \n",
      "\n",
      "       PredictionClickbait  \n",
      "26098            90.333954  \n",
      "27455            76.749886  \n",
      "28564            99.275497  \n",
      "29220            99.993721  \n",
      "26119            99.993828  \n",
      "\n",
      "Total number of errors: 219\n",
      "\n",
      "Number of correctly predicted entries with a prediction <= 5%: 15717\n",
      "Number of correctly predicted entries with a prediction >= 95%: 15983\n",
      "Total: 31700\n"
     ]
    }
   ],
   "source": [
    "# Get the model's predictions on the dataset\n",
    "predictions = model.predict([padded, extra_features])\n",
    "predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compare the predictions with the actual labels\n",
    "errors = dataset[predicted_labels != dataset['clickbait']].copy()\n",
    "\n",
    "# Add the prediction percentage to the errors DataFrame\n",
    "errors['PredictionClickbait'] = predictions[predicted_labels != dataset['clickbait']] * 100\n",
    "\n",
    "# Display 5 random errors\n",
    "print(\"5 Error Samples\")\n",
    "print(errors[['headline', 'clickbait', 'PredictionClickbait']].sample(5))\n",
    "# Display the total number of errors\n",
    "print(f'\\nTotal number of errors: {len(errors)}')\n",
    "\n",
    "# Count the number of correctly predicted entries with a prediction <= 5% and >= 95%\n",
    "correct_no_clickbait = len(dataset[(predictions.flatten() <= 0.05) & (dataset['clickbait'] == 0)])\n",
    "correct_clickbait = len(dataset[(predictions.flatten() >= 0.95) & (dataset['clickbait'] == 1)])\n",
    "\n",
    "print(f'\\nNumber of correctly predicted entries with a prediction <= 5%: {correct_no_clickbait}')\n",
    "print(f'Number of correctly predicted entries with a prediction >= 95%: {correct_clickbait}')\n",
    "print(f'Total: {correct_no_clickbait + correct_clickbait}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
